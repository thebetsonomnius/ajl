---

title: "initiation"

date: 2023-04-18

---

<!-- more -->

It is regrettable that the initiation of the slow takeoff apparently requires the mass privatization of top-shelf intellects. It is not however unpredictable - previous technological movements in American life were preceded by the brief relocation of the intellectual elite to locked rooms. The Title 50-ing of algebraic topologists and combinatorics theorists in the 1970s and 80s to work on cryptographic projects for the NSA's then-nascent scalable surveillance program led quite directly to the dot-com boom of the 90s. In the 40s, every German-trained physicist in the world was placed under lock and key in the American southwest to build devices that produced the 20th century world order and much of the peacetime technological innovations that spawned the digital revolution. But, neither the dot-com boom nor the commercialization of nuclear missile technology live on the same order of magnitude as the advent of [Yemlems](https://ajl.bio/definitions#yemlems). The distinction in significance has thus far not been accompanied by a similar increase in the quality of the public discourse. This is due to the rapid move to venture-capitalize the previously open-source research labs at the frontier of the [field](https://ajl.bio/definitions#deep-learning). Privatization in deep learning is by no means a novel phenomenon - in 2017, Qualcomm [effectively gutted](https://www.qualcomm.com/news/releases/2017/08/qualcomm-bolsters-position-artificial-intelligence-research-outlines-its) one of the world's most illustrious particle physics departments in their pursuit of deep learning research talent. What is different in the current wave of close-sourcing is the power of the technology being restricted and the profile of the people producing it. 

In previous deep learning privatizations, old legends of the field with no pretensions to open-source fundamentalism gathered their most dependable graduates into a high-prestige package to be sold to the highest bidder. What was being purchased by the corporations wasn't technology - it was connections and landscape knowledge of a then-foreign and poorly mapped field. This time around, it's the 20-somethings that are leading the charge, and the venture capitalists across the table know that what they're buying is not prestige, but implementation wisdom. Training a Yemlem is a high-stakes affair. Compute costs run well into the 10s of millions, the return on capital is low, and the training runs themselves take months to execute. While it is not [all that difficult](https://ajl.bio/2023/04/20/so-you-want-to-train-a-yemlem.html) to train a Yemlem if one is well-versed, VCs face a very high risk profile in taking a $90 million dollar 6-month gamble on a trio of twenty-somethings who claim they can build Skynet. Anyone with a proven record of achievement even tangentially related to the design and manufacture of Yemlems is therefore a multimillion dollar commodity, due merely to the anxiolytic effect their resume has on whoever's hiring or funding them. Money has a way of holding the attention of those receiving it, particularly when the recipient is a research scientist working in a previously capital-bottlenecked field who's suddenly been granted unsupervised access to a billion-dollar bank account.

The intellectual vacuum left behind by the rapid exit of the top young minds from public intellectual discourse has been filled by a somewhat amorphous group I've termed the [Yuddites](https://ajl.bio/definitions#Yuddite). Yuddites share in common an affinity for [blogging](https://lesswrong.com), ignoring mathematics, [Kurzweil syndrome](https://ajl.bio/definitions#kurzweil-syndrome), and [voiding their life's work without realizing it](https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators). At their best, the Yuddites were the last post-Uber IPO vanguard of intellectual adventurism west of the Mississippi. Today, they are purveyors of [Bostromite](https://nickbostrom.com) nonsense who corrupt the public discourse on AGI by spewing ill-founded and self-contradictory nonsense into the societal medium.

There is nothing false about the intrinsic moral motivations of the Yuddite program - AGI is imminent, and it does pose a serious threat to human civilization. It is in confidently speculating on the nature of that threat that the Yuddites run astray; they are not mathematically equipped to make such judgments. The deep learning research program is not re-normalizable. That is, the accuracy of one's predictions about it isn't a smoothly varying function of your knowledge - it's a step function. You either understand it holistically, or you don't. There are many competing hermeneutic and analytic strategies for grokking deep learning - knowing which to fall back on for a given problem is something that can only be learned by engaging in research at the edge for an extended period of time. None of this is to say that deep learning is mystical or amorphous. It admits a plethora of mathematically and empirically rigorous analyses. Unfortunately, the field has been placed in the position of having to holistically articulate its various analytic methods in the public sphere to a lay audience to which the subject matter is completely foreign. Its failure to do so (in no small part due to its luminaries' occupation with more productive pursuits) has forced a subculture unequipped to take on the role of expositer to become the point of exchange between the field and the public.

The primary goal of this series of posts will be to deconstruct the fallacious elements of the Yuddites' arguments, in a form digestible by the target audience. The secondary goal, to be pursued simultaneously, will be to reconstruct the Yuddites' intellectual program in a form consonant with the relevant mathematical and empirical results. AGI is in fact a clear and present danger, regardless of the extent to which the nature of the threat and its dynamical properties have been misconstrued.

"initiation" will be the dryest of the posts in this series - it's really more of a syllabus than a post. I anticipate a semi-daily post rate, not subject to extended recursions related to [terminology definitions](https://ajl.bio/definitions), and dev sprints for [indxd](https://indxd.co) & forwardpass. At some point in the next week, I'm going to embed a chat oracle on the site, and a hybrid keyword-neural retriever powered by indxd. It'll be secure - don't go endpoint panning in the bundle. Unless you're a pro pen tester (or are part of some imaginary working group at Adept fine-tuning active [Zemlems](https://ajl.bio/definitions#zemlems) on Phrack), you're better off just checking out [Dust](https://dust.tt) - it's a sieve. In addition, I'm writing an actively maintained [best practices](https://ajl.bio/2023/04/20/so-you-want-to-train-a-yemlem.html) on training foundation Yemlems, and a (somewhat skeletal atm) [active research program](https://ajl.bio/manifesting-kakade.html) for minimal-cost Yemlem alignment.

The first 2 posts in this series will be: 
1. > [Residual Streams, Stochastic Depth Regularization, and Gradient Hacking](https://ajl.bio/2023/04/20/residual-streams-and-gradient-hacking.html)
2. > [NP Completeness and Yemlem Alignment](https://ajl.bio/2023/04/22/np-completeness-and-alignment.html)

