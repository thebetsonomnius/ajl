---

title: "initiation"

date: 2023-04-18

---

<!-- more -->

It is regrettable that the initiation of the [slow takeoff](https://www.lesswrong.com/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff#:~:text=He%20defines%20slow%20takeoff%20as,negation%20of%20the%20above%20statement.) apparently requires the mass privatization of top-shelf deep learning talent. It is not however unpredictable - previous technological movements in American life were preceded by the brief relocation of a small intellectual elite to restricted sites. The annexation by the NSA in the 70s and 80s of the algebraic topology and combinatorics departments at SUNY Stony Brook comes to mind. The techniques pioneered by that group under a veil of secrecy eventually enabled the massive opening of computer networks to unvetted traffic in the 1990s. However, the scale of the boom that produced will be dwarfed by the effect [Yemlems](https://ajl.bio/definitions#yemlems) will have on human civilization. The dot-com revolution shortened the maximum connection latency between any 2 intelligences to a few seconds. Yemlems shorten that to the HBM access latency, which is on the order of microseconds. The scale of the impending phase change in civilizational dynamics demands in the first place a clear public understanding of the perils. Without it, policy makers cannot be made to act, unless forced by catastrophe.

The intellectual vacuum left behind by the rapid privatization of the top talent from the field has been filled by a somewhat amorphous group I've termed the [Yuddites](https://ajl.bio/definitions#Yuddite). Yuddites share in common an affinity for [blogging about effective altruism](https://lesswrong.com), ignoring mathematics, and [contradicting their own arguments unironically](https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators). To give them their due, the Yuddites were at one time the last vanguard of intellectual adventurism west of the Mississippi, in the dark interregnum between the Uber IPO exodus and the [Cerebral Valley](https://cerebralvalley.ai/) revival. Today, they are purveyors of [Bostromite](https://nickbostrom.com) nonsense who hopelessly confuse the public discourse on AGI by spewing ill-founded and self-contradictory arguments into the societal medium. Don't misunderstand - all of this is well meaning. The problem with the Yuddites is not their motivations but their overconfident and fallacious predictions.

There is nothing false about the moral ends of the Yuddite program - quite the contrary. AGI is imminent, and it does pose a serious threat to human civilization. It is in confidently speculating on the nature of that threat that the Yuddites run astray; they are not mathematically equipped to make such judgments. The deep learning research program is not re-normalizable. That is, the accuracy of one's predictions about the field is not a smoothly varying function of your knowledge - it's a step function. You either understand it holistically, or you don't. There are many competing hermeneutic and analytic frameworks for modeling deep learning - knowing which to fall back on in any given instance is something that can only be learned by engaging in research at the edge for an extended period of time. None of this is to say that deep learning is mystical or amorphous. It admits a plethora of mathematically and empirically rigorous analyses. Unfortunately, the field has been placed in the position of having to holistically articulate its various analytic methods to a lay audience to which the subject matter is completely foreign. Its failure to do so has led a subculture unequipped to take on the role of expositer to become the point of exchange between the field and the public.

The primary goal of this series of posts will be to deconstruct the fallacious elements of the Yuddites' arguments, in a form digestible by the obvious audience. The secondary goal, to be pursued simultaneously, will be to elaborate on the true nature of the takeoff and the perils associated with it. AGI is in fact a clear and present danger, regardless of the extent to which the nature of the threat and its dynamical properties have been misconstrued.

"initiation" will be the dryest of the posts in this series - it's really more of a syllabus than a post. I anticipate a semi-daily post rate, not subject to extended recursions related to [terminology definitions](https://ajl.bio/definitions) and dev sprints for [indxd](https://indxd.co) & forwardpass. At some point in the next week, I'm going to embed a chat oracle on the site, and a hybrid keyword-neural retriever powered by indxd. It'll be secure. Please don't go endpoint panning in the bundle. Unless you're a pro pen tester (or are part of some secret working group at Adept fine-tuning active [Zemlems](https://ajl.bio/definitions#zemlems) on Phrack), you're better off just checking out [Dust](https://dust.tt). It's a sieve. In addition, I'm writing an actively maintained [best practices](https://ajl.bio/2023/04/29/so-you-want-to-train-a-yemlem.html) on training foundation Yemlems, and a (somewhat skeletal atm) [active research program](https://ajl.bio/2023/04/29/pleasing-kakade.html) for minimal-cost Yemlem alignment. These will be published by late April.

The first 2 posts in this series will be: 
1. > [Residual Streams, Stochastic Depth Regularization, and Gradient Hacking](https://ajl.bio/2023/04/20/residual-streams-and-gradient-hacking.html)
2. > [Zipfian Distributions and Yemlem Alignment](https://ajl.bio/2023/04/22/zipfian-distributions-and-yemlem-alignment.html)

