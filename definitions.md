---
title: Definitions
permalink: /definitions/
---

## deep learning
A neural network, in the simple limit, is a stack of interleaved linear operators and nonlinear functions. In computer science, we impose the additional constraint that the nonlinear functions be differentiable. The linear operator is differentiable precisely because it is linear. Because both the linear operator and the nonlinear function are differentiable, their composition is also differentiable - this follows from the [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule). A deep neural network is a stack of these linear operator-nonlinear function sequences. Because all individual components of the deep neural network are differentiable, the deep neural network as a whole is differentiable. That's why deep learning is possible. Backpropagation, the process by which we compute the deep neural network's derivative, is the fastest possible error propagation algorithm we know of, and it's not a close contest. It is an abstract, sped-up version of the information processing system implemented in the neo-cortex. Regrettably, modern deep learning is not so simple. In order to scale deep learning to networks composed of hundreds of layers, each with linear operators embedded in tens of thousands of dimensions, we've had to make a series of haphazard technical additions to the formula. In (very) rough order of importance, they are: residual streams, L-p penalties, layer normalization, dynamically isometric initializations, adaptive moment optimizers, stochastic depth regularization, entropy re-parameterization, truncated normal initializations, principled nonlinearity selection, and cosine learning rate scheduling. Combined together, these elements enable the training of arbitrarily large neural networks. As of April 2023, the largest neural networks in existence are [Yemlems](#yemlems) - networks requiring 1 or more yottaFLOPs to train to convergence.

## zemlems
A zemlem is a ZettascalE MultimodaL Model (the 2nd e is inserted for phonetic consistency). The replacement of language with multimodal is for posterity's sake - some models are language-only, and some aren't, but it follows from the [circuit diversity theory](#circuit-diversity-theory) that Transformer generalization increases with the diversity of the training data. It is therefore safe to assume that in several years' time, LLMs will be completely replaced by LMMs. Zettascale refers here to the number of floating point operations required to train the model. With the exception of PaLM 540B and likely GPT-4-vision (which are [yemlems](#yemlems)), most of the largest models in existence today are zemlems. They are trained on clusters with theoretical max throughput of 1-25 exaFLOPS.

## yemlems
A yemlem is a YottascalE Multimodal Model (the 2nd e is inserted for phonetic consistency). The replacement of language with multimodal is for posterity's sake - some models are language-only, and some aren't, but it follows from the [circuit diversity theory](#circuit-diversity-theory) that Transformer generalization increases with the diversity of the training data. It is therefore safe to assume that in several years' time, LLMs will be completely replaced by LMMs. Yottascale refers here to the number of floating point operations required to train the model. This class is quite sparsely populated at the moment. The only public accessible models confirmed to fall in this class are PaLM 540B (per the training wall-clock time stated by Google multiplied by the realized FLOP throughput of the dual TPUv4 pods), and GPT-4-vision (per Sam Altman's comments made at MIT on the dollar-cost of training the GPT-4 family).

## the pure language limit
Per calculations based on the [separation rank of decoder-only Transformers](https://arxiv.org/pdf/2105.03928.pdf), and the hypothesized amount of high-volume or high-quality tokens on the internet, there are fundamental limits to the compute and parameter count scaling laws of LLMs. If one assumes the scaling laws derived from the separation rank figures linked to in the previous sentence, the largest LLM one can profitably train is between 1T-1.3T parameters. I presume a constant vocabulary size of ~32K, due to the theoretical and [apparently practical](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) improvements in generalization at the expense of inference speed that come from a narrower vocabulary.

## circuit diversity theory
Neural networks compose circuits as they learn. If there are no circuits available to be composed into a circuit that can increase the model evidence, the model will try to memorize the data. Increasing circuit diversity in a model increases the probability that some combination of the circuits will increase model evidence. This is why bigger models trained on more diverse data generalize better than smaller models trained on less diverse data. Capabilities that emerge with increasing model size, training steps, and data diversity do so because of such models' higher circuit diversity compared to models with smaller size, trained for fewer steps, on less diverse data.

## yuddites
The intellectual progeny of Eliezer Yudkowsky. Alternatively defined as anyone who makes unfounded and poorly reasoned conjectures on AGI. Statistically speaking, most people who've uttered the phrase AGI fall under this category.
